{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4 - Question Duplicates\n",
    "\n",
    "Below for reference:\n",
    "\n",
    "https://github.com/latentghost/NLP-Specialization-deeplearning.ai/blob/818304cecbe2d64252c365eb2ad9f350676ffd1b/NLP%20with%20Sequence%20Models/Week3/Assignment/.ipynb_checkpoints/C3W3_Assignment-checkpoint.ipynb#L989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## Overview\n",
    "In this assignment, concretely you will: \n",
    "\n",
    "- Learn about Siamese networks\n",
    "- Understand how the triplet loss works\n",
    "- Understand how to evaluate accuracy\n",
    "- Use cosine similarity between the model's outputted vectors\n",
    "- Use the data generator to get batches of questions\n",
    "- Predict using your own model\n",
    "\n",
    "By now, you are familiar with trax and know how to make use of classes to define your model. We will start this homework by asking you to preprocess the data the same way you did in the previous assignments. After processing the data you will build a classifier that will allow you to identify whether two questions are the same or not. \n",
    "<img src = \"images/meme.png\" style=\"width:550px;height:300px;\"/>\n",
    "\n",
    "\n",
    "You will process the data first and then pad in a similar way you have done in the previous assignment. Your model will take in the two question embeddings, run them through an LSTM, and then compare the outputs of the two sub networks using cosine similarity. Before taking a deep dive into the model, start by importing the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds\n",
    "rnd.seed(34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import w3_unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of questions pairs: 404351\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data/questions.csv')\n",
    "print(f\"number of questions pairs: {len(data)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of training set: 300000, length of testing set: 10240\n"
     ]
    }
   ],
   "source": [
    "N_TRAIN = 300000\n",
    "N_TEST=10*1024\n",
    "data_train = data[:N_TRAIN]\n",
    "data_test = data[N_TRAIN:N_TRAIN + N_TEST]\n",
    "\n",
    "print(f\"length of training set: {len(data_train)}, length of testing set: {len(data_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 7,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 15,\n",
       " 16,\n",
       " 18,\n",
       " 20,\n",
       " 29,\n",
       " 31,\n",
       " 32,\n",
       " 38,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 53,\n",
       " 58,\n",
       " 62,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 79,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 88,\n",
       " 92,\n",
       " 93,\n",
       " 95,\n",
       " 100,\n",
       " 104,\n",
       " 107,\n",
       " 113,\n",
       " 120,\n",
       " 122,\n",
       " 125,\n",
       " 127,\n",
       " 135,\n",
       " 136,\n",
       " 143,\n",
       " 144,\n",
       " 152,\n",
       " 156,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 163,\n",
       " 165,\n",
       " 168,\n",
       " 173,\n",
       " 175,\n",
       " 176,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 182,\n",
       " 185,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 193,\n",
       " 194,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 203,\n",
       " 209,\n",
       " 210,\n",
       " 215,\n",
       " 216,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 224,\n",
       " 226,\n",
       " 229,\n",
       " 235,\n",
       " 236,\n",
       " 238,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 246,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 253,\n",
       " 255,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 267,\n",
       " 269,\n",
       " 270,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 281,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 291,\n",
       " 293,\n",
       " 295,\n",
       " 296,\n",
       " 299,\n",
       " 304,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 312,\n",
       " 317,\n",
       " 318,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 326,\n",
       " 329,\n",
       " 331,\n",
       " 339,\n",
       " 341,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 353,\n",
       " 364,\n",
       " 365,\n",
       " 368,\n",
       " 373,\n",
       " 377,\n",
       " 380,\n",
       " 383,\n",
       " 390,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 397,\n",
       " 399,\n",
       " 400,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 409,\n",
       " 410,\n",
       " 412,\n",
       " 415,\n",
       " 421,\n",
       " 422,\n",
       " 428,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 439,\n",
       " 442,\n",
       " 443,\n",
       " 445,\n",
       " 446,\n",
       " 450,\n",
       " 451,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 464,\n",
       " 468,\n",
       " 476,\n",
       " 479,\n",
       " 483,\n",
       " 484,\n",
       " 488,\n",
       " 489,\n",
       " 491,\n",
       " 493,\n",
       " 501,\n",
       " 503,\n",
       " 505,\n",
       " 506,\n",
       " 507,\n",
       " 508,\n",
       " 513,\n",
       " 515,\n",
       " 517,\n",
       " 518,\n",
       " 520,\n",
       " 527,\n",
       " 532,\n",
       " 533,\n",
       " 534,\n",
       " 535,\n",
       " 536,\n",
       " 538,\n",
       " 540,\n",
       " 544,\n",
       " 547,\n",
       " 549,\n",
       " 551,\n",
       " 553,\n",
       " 555,\n",
       " 557,\n",
       " 558,\n",
       " 561,\n",
       " 564,\n",
       " 569,\n",
       " 571,\n",
       " 573,\n",
       " 577,\n",
       " 580,\n",
       " 583,\n",
       " 584,\n",
       " 586,\n",
       " 590,\n",
       " 592,\n",
       " 593,\n",
       " 594,\n",
       " 595,\n",
       " 600,\n",
       " 602,\n",
       " 607,\n",
       " 608,\n",
       " 609,\n",
       " 617,\n",
       " 618,\n",
       " 619,\n",
       " 620,\n",
       " 625,\n",
       " 626,\n",
       " 630,\n",
       " 636,\n",
       " 637,\n",
       " 641,\n",
       " 644,\n",
       " 647,\n",
       " 648,\n",
       " 650,\n",
       " 652,\n",
       " 654,\n",
       " 656,\n",
       " 658,\n",
       " 660,\n",
       " 661,\n",
       " 663,\n",
       " 664,\n",
       " 665,\n",
       " 675,\n",
       " 676,\n",
       " 678,\n",
       " 680,\n",
       " 688,\n",
       " 689,\n",
       " 696,\n",
       " 699,\n",
       " 700,\n",
       " 702,\n",
       " 704,\n",
       " 705,\n",
       " 707,\n",
       " 713,\n",
       " 721,\n",
       " 722,\n",
       " 726,\n",
       " 727,\n",
       " 728,\n",
       " 729,\n",
       " 734,\n",
       " 735,\n",
       " 739,\n",
       " 742,\n",
       " 746,\n",
       " 750,\n",
       " 752,\n",
       " 755,\n",
       " 760,\n",
       " 761,\n",
       " 766,\n",
       " 772,\n",
       " 773,\n",
       " 779,\n",
       " 781,\n",
       " 784,\n",
       " 785,\n",
       " 787,\n",
       " 790,\n",
       " 791,\n",
       " 796,\n",
       " 798,\n",
       " 800,\n",
       " 803,\n",
       " 805,\n",
       " 806,\n",
       " 812,\n",
       " 815,\n",
       " 817,\n",
       " 819,\n",
       " 825,\n",
       " 827,\n",
       " 828,\n",
       " 830,\n",
       " 834,\n",
       " 835,\n",
       " 836,\n",
       " 837,\n",
       " 840,\n",
       " 848,\n",
       " 849,\n",
       " 850,\n",
       " 852,\n",
       " 855,\n",
       " 856,\n",
       " 858,\n",
       " 859,\n",
       " 861,\n",
       " 863,\n",
       " 869,\n",
       " 872,\n",
       " 874,\n",
       " 876,\n",
       " 877,\n",
       " 880,\n",
       " 889,\n",
       " 890,\n",
       " 891,\n",
       " 892,\n",
       " 893,\n",
       " 898,\n",
       " 903,\n",
       " 904,\n",
       " 905,\n",
       " 906,\n",
       " 907,\n",
       " 908,\n",
       " 915,\n",
       " 916,\n",
       " 917,\n",
       " 918,\n",
       " 919,\n",
       " 922,\n",
       " 923,\n",
       " 929,\n",
       " 930,\n",
       " 934,\n",
       " 938,\n",
       " 939,\n",
       " 943,\n",
       " 945,\n",
       " 946,\n",
       " 949,\n",
       " 951,\n",
       " 952,\n",
       " 953,\n",
       " 954,\n",
       " 956,\n",
       " 958,\n",
       " 959,\n",
       " 962,\n",
       " 966,\n",
       " 967,\n",
       " 968,\n",
       " 969,\n",
       " 973,\n",
       " 980,\n",
       " 983,\n",
       " 985,\n",
       " 989,\n",
       " 991,\n",
       " 998,\n",
       " 1001,\n",
       " 1002,\n",
       " 1004,\n",
       " 1005,\n",
       " 1006,\n",
       " 1007,\n",
       " 1010,\n",
       " 1013,\n",
       " 1014,\n",
       " 1015,\n",
       " 1018,\n",
       " 1020,\n",
       " 1023,\n",
       " 1024,\n",
       " 1030,\n",
       " 1033,\n",
       " 1036,\n",
       " 1039,\n",
       " 1043,\n",
       " 1049,\n",
       " 1051,\n",
       " 1053,\n",
       " 1054,\n",
       " 1057,\n",
       " 1062,\n",
       " 1073,\n",
       " 1075,\n",
       " 1086,\n",
       " 1088,\n",
       " 1093,\n",
       " 1103,\n",
       " 1106,\n",
       " 1108,\n",
       " 1109,\n",
       " 1110,\n",
       " 1114,\n",
       " 1115,\n",
       " 1118,\n",
       " 1121,\n",
       " 1127,\n",
       " 1128,\n",
       " 1129,\n",
       " 1132,\n",
       " 1134,\n",
       " 1136,\n",
       " 1137,\n",
       " 1140,\n",
       " 1141,\n",
       " 1142,\n",
       " 1150,\n",
       " 1152,\n",
       " 1153,\n",
       " 1154,\n",
       " 1156,\n",
       " 1157,\n",
       " 1158,\n",
       " 1164,\n",
       " 1165,\n",
       " 1166,\n",
       " 1167,\n",
       " 1171,\n",
       " 1172,\n",
       " 1174,\n",
       " 1175,\n",
       " 1185,\n",
       " 1186,\n",
       " 1189,\n",
       " 1190,\n",
       " 1191,\n",
       " 1192,\n",
       " 1198,\n",
       " 1204,\n",
       " 1208,\n",
       " 1209,\n",
       " 1210,\n",
       " 1214,\n",
       " 1216,\n",
       " 1218,\n",
       " 1223,\n",
       " 1225,\n",
       " 1228,\n",
       " 1231,\n",
       " 1233,\n",
       " 1238,\n",
       " 1239,\n",
       " 1241,\n",
       " 1242,\n",
       " 1244,\n",
       " 1248,\n",
       " 1249,\n",
       " 1254,\n",
       " 1255,\n",
       " 1258,\n",
       " 1259,\n",
       " 1260,\n",
       " 1264,\n",
       " 1268,\n",
       " 1269,\n",
       " 1270,\n",
       " 1271,\n",
       " 1272,\n",
       " 1273,\n",
       " 1274,\n",
       " 1280,\n",
       " 1285,\n",
       " 1287,\n",
       " 1291,\n",
       " 1294,\n",
       " 1302,\n",
       " 1303,\n",
       " 1307,\n",
       " 1311,\n",
       " 1313,\n",
       " 1314,\n",
       " 1315,\n",
       " 1318,\n",
       " 1319,\n",
       " 1324,\n",
       " 1325,\n",
       " 1326,\n",
       " 1328,\n",
       " 1329,\n",
       " 1332,\n",
       " 1333,\n",
       " 1335,\n",
       " 1343,\n",
       " 1345,\n",
       " 1346,\n",
       " 1347,\n",
       " 1356,\n",
       " 1360,\n",
       " 1363,\n",
       " 1374,\n",
       " 1379,\n",
       " 1383,\n",
       " 1384,\n",
       " 1387,\n",
       " 1388,\n",
       " 1389,\n",
       " 1393,\n",
       " 1397,\n",
       " 1398,\n",
       " 1399,\n",
       " 1400,\n",
       " 1401,\n",
       " 1403,\n",
       " 1406,\n",
       " 1409,\n",
       " 1411,\n",
       " 1413,\n",
       " 1414,\n",
       " 1415,\n",
       " 1416,\n",
       " 1417,\n",
       " 1418,\n",
       " 1425,\n",
       " 1426,\n",
       " 1429,\n",
       " 1433,\n",
       " 1436,\n",
       " 1438,\n",
       " 1443,\n",
       " 1445,\n",
       " 1446,\n",
       " 1449,\n",
       " 1451,\n",
       " 1452,\n",
       " 1454,\n",
       " 1456,\n",
       " 1458,\n",
       " 1462,\n",
       " 1464,\n",
       " 1466,\n",
       " 1471,\n",
       " 1473,\n",
       " 1474,\n",
       " 1475,\n",
       " 1476,\n",
       " 1477,\n",
       " 1480,\n",
       " 1490,\n",
       " 1491,\n",
       " 1496,\n",
       " 1499,\n",
       " 1500,\n",
       " 1504,\n",
       " 1506,\n",
       " 1509,\n",
       " 1515,\n",
       " 1516,\n",
       " 1520,\n",
       " 1530,\n",
       " 1534,\n",
       " 1537,\n",
       " 1539,\n",
       " 1540,\n",
       " 1541,\n",
       " 1543,\n",
       " 1544,\n",
       " 1549,\n",
       " 1557,\n",
       " 1559,\n",
       " 1562,\n",
       " 1563,\n",
       " 1565,\n",
       " 1567,\n",
       " 1568,\n",
       " 1569,\n",
       " 1573,\n",
       " 1576,\n",
       " 1579,\n",
       " 1582,\n",
       " 1583,\n",
       " 1585,\n",
       " 1592,\n",
       " 1593,\n",
       " 1594,\n",
       " 1595,\n",
       " 1597,\n",
       " 1600,\n",
       " 1605,\n",
       " 1607,\n",
       " 1610,\n",
       " 1612,\n",
       " 1624,\n",
       " 1630,\n",
       " 1633,\n",
       " 1637,\n",
       " 1639,\n",
       " 1641,\n",
       " 1645,\n",
       " 1649,\n",
       " 1655,\n",
       " 1656,\n",
       " 1657,\n",
       " 1662,\n",
       " 1664,\n",
       " 1668,\n",
       " 1670,\n",
       " 1672,\n",
       " 1676,\n",
       " 1677,\n",
       " 1678,\n",
       " 1682,\n",
       " 1683,\n",
       " 1687,\n",
       " 1689,\n",
       " 1690,\n",
       " 1691,\n",
       " 1692,\n",
       " 1694,\n",
       " 1695,\n",
       " 1696,\n",
       " 1698,\n",
       " 1702,\n",
       " 1704,\n",
       " 1705,\n",
       " 1706,\n",
       " 1711,\n",
       " 1712,\n",
       " 1717,\n",
       " 1719,\n",
       " 1722,\n",
       " 1728,\n",
       " 1734,\n",
       " 1737,\n",
       " 1739,\n",
       " 1740,\n",
       " 1746,\n",
       " 1747,\n",
       " 1749,\n",
       " 1751,\n",
       " 1752,\n",
       " 1757,\n",
       " 1759,\n",
       " 1761,\n",
       " 1762,\n",
       " 1768,\n",
       " 1778,\n",
       " 1779,\n",
       " 1780,\n",
       " 1784,\n",
       " 1786,\n",
       " 1789,\n",
       " 1790,\n",
       " 1791,\n",
       " 1795,\n",
       " 1796,\n",
       " 1803,\n",
       " 1805,\n",
       " 1806,\n",
       " 1815,\n",
       " 1816,\n",
       " 1820,\n",
       " 1825,\n",
       " 1835,\n",
       " 1839,\n",
       " 1842,\n",
       " 1846,\n",
       " 1850,\n",
       " 1851,\n",
       " 1852,\n",
       " 1854,\n",
       " 1858,\n",
       " 1861,\n",
       " 1862,\n",
       " 1868,\n",
       " 1870,\n",
       " 1873,\n",
       " 1875,\n",
       " 1879,\n",
       " 1883,\n",
       " 1890,\n",
       " 1891,\n",
       " 1892,\n",
       " 1893,\n",
       " 1896,\n",
       " 1897,\n",
       " 1899,\n",
       " 1901,\n",
       " 1904,\n",
       " 1905,\n",
       " 1906,\n",
       " 1907,\n",
       " 1909,\n",
       " 1910,\n",
       " 1911,\n",
       " 1913,\n",
       " 1918,\n",
       " 1919,\n",
       " 1922,\n",
       " 1924,\n",
       " 1929,\n",
       " 1933,\n",
       " 1938,\n",
       " 1939,\n",
       " 1941,\n",
       " 1943,\n",
       " 1947,\n",
       " 1948,\n",
       " 1951,\n",
       " 1955,\n",
       " 1958,\n",
       " 1959,\n",
       " 1960,\n",
       " 1961,\n",
       " 1968,\n",
       " 1972,\n",
       " 1973,\n",
       " 1977,\n",
       " 1979,\n",
       " 1980,\n",
       " 1981,\n",
       " 1983,\n",
       " 1985,\n",
       " 1987,\n",
       " 1988,\n",
       " 1989,\n",
       " 1990,\n",
       " 1991,\n",
       " 1992,\n",
       " 1998,\n",
       " 2001,\n",
       " 2005,\n",
       " 2006,\n",
       " 2007,\n",
       " 2010,\n",
       " 2011,\n",
       " 2012,\n",
       " 2013,\n",
       " 2018,\n",
       " 2019,\n",
       " 2022,\n",
       " 2025,\n",
       " 2027,\n",
       " 2029,\n",
       " 2030,\n",
       " 2033,\n",
       " 2034,\n",
       " 2035,\n",
       " 2036,\n",
       " 2039,\n",
       " 2043,\n",
       " 2046,\n",
       " 2048,\n",
       " 2050,\n",
       " 2053,\n",
       " 2057,\n",
       " 2061,\n",
       " 2062,\n",
       " 2063,\n",
       " 2069,\n",
       " 2071,\n",
       " 2072,\n",
       " 2076,\n",
       " 2077,\n",
       " 2078,\n",
       " 2080,\n",
       " 2081,\n",
       " 2086,\n",
       " 2087,\n",
       " 2096,\n",
       " 2097,\n",
       " 2098,\n",
       " 2100,\n",
       " 2101,\n",
       " 2102,\n",
       " 2103,\n",
       " 2104,\n",
       " 2106,\n",
       " 2108,\n",
       " 2110,\n",
       " 2112,\n",
       " 2113,\n",
       " 2116,\n",
       " 2117,\n",
       " 2122,\n",
       " 2132,\n",
       " 2136,\n",
       " 2141,\n",
       " 2143,\n",
       " 2145,\n",
       " 2148,\n",
       " 2152,\n",
       " 2157,\n",
       " 2160,\n",
       " 2164,\n",
       " 2165,\n",
       " 2166,\n",
       " 2170,\n",
       " 2173,\n",
       " 2174,\n",
       " 2176,\n",
       " 2177,\n",
       " 2179,\n",
       " 2184,\n",
       " 2185,\n",
       " 2186,\n",
       " 2187,\n",
       " 2188,\n",
       " 2189,\n",
       " 2190,\n",
       " 2191,\n",
       " 2192,\n",
       " 2195,\n",
       " 2196,\n",
       " 2201,\n",
       " 2203,\n",
       " 2204,\n",
       " 2207,\n",
       " 2208,\n",
       " 2209,\n",
       " 2214,\n",
       " 2215,\n",
       " 2217,\n",
       " 2218,\n",
       " 2219,\n",
       " 2223,\n",
       " 2224,\n",
       " 2225,\n",
       " 2228,\n",
       " 2231,\n",
       " 2233,\n",
       " 2234,\n",
       " 2235,\n",
       " 2237,\n",
       " 2239,\n",
       " 2244,\n",
       " 2245,\n",
       " 2247,\n",
       " 2250,\n",
       " 2251,\n",
       " 2252,\n",
       " 2255,\n",
       " 2258,\n",
       " 2260,\n",
       " 2261,\n",
       " 2264,\n",
       " 2266,\n",
       " 2271,\n",
       " 2272,\n",
       " 2275,\n",
       " 2276,\n",
       " 2278,\n",
       " 2282,\n",
       " 2285,\n",
       " 2287,\n",
       " 2291,\n",
       " 2302,\n",
       " 2303,\n",
       " 2304,\n",
       " 2305,\n",
       " 2306,\n",
       " 2309,\n",
       " 2313,\n",
       " 2315,\n",
       " 2317,\n",
       " 2319,\n",
       " 2326,\n",
       " 2327,\n",
       " 2328,\n",
       " 2334,\n",
       " 2335,\n",
       " 2338,\n",
       " 2341,\n",
       " 2344,\n",
       " 2345,\n",
       " 2347,\n",
       " 2357,\n",
       " 2358,\n",
       " 2362,\n",
       " 2365,\n",
       " 2370,\n",
       " 2372,\n",
       " 2377,\n",
       " 2378,\n",
       " 2379,\n",
       " 2381,\n",
       " 2383,\n",
       " 2384,\n",
       " 2390,\n",
       " 2391,\n",
       " 2392,\n",
       " 2393,\n",
       " 2396,\n",
       " 2397,\n",
       " 2400,\n",
       " 2402,\n",
       " 2403,\n",
       " 2404,\n",
       " 2405,\n",
       " 2407,\n",
       " 2409,\n",
       " 2414,\n",
       " 2417,\n",
       " 2420,\n",
       " 2421,\n",
       " 2422,\n",
       " 2423,\n",
       " 2426,\n",
       " 2430,\n",
       " 2431,\n",
       " 2432,\n",
       " 2433,\n",
       " 2438,\n",
       " 2443,\n",
       " 2449,\n",
       " 2450,\n",
       " 2451,\n",
       " 2452,\n",
       " 2453,\n",
       " 2455,\n",
       " 2456,\n",
       " 2458,\n",
       " 2459,\n",
       " 2463,\n",
       " 2466,\n",
       " 2467,\n",
       " 2468,\n",
       " 2470,\n",
       " 2471,\n",
       " 2472,\n",
       " 2475,\n",
       " 2477,\n",
       " 2478,\n",
       " 2479,\n",
       " 2487,\n",
       " 2490,\n",
       " 2491,\n",
       " 2494,\n",
       " 2496,\n",
       " 2502,\n",
       " 2503,\n",
       " 2505,\n",
       " 2511,\n",
       " 2512,\n",
       " 2513,\n",
       " 2516,\n",
       " 2517,\n",
       " 2519,\n",
       " 2520,\n",
       " 2523,\n",
       " 2526,\n",
       " 2530,\n",
       " 2535,\n",
       " 2536,\n",
       " 2537,\n",
       " 2539,\n",
       " 2541,\n",
       " 2544,\n",
       " 2545,\n",
       " 2546,\n",
       " 2547,\n",
       " 2548,\n",
       " 2551,\n",
       " 2554,\n",
       " 2561,\n",
       " 2562,\n",
       " 2563,\n",
       " 2564,\n",
       " 2566,\n",
       " 2569,\n",
       " 2571,\n",
       " 2576,\n",
       " 2577,\n",
       " 2582,\n",
       " 2584,\n",
       " 2585,\n",
       " 2587,\n",
       " 2588,\n",
       " 2589,\n",
       " 2595,\n",
       " 2596,\n",
       " 2597,\n",
       " 2599,\n",
       " 2601,\n",
       " 2606,\n",
       " 2613,\n",
       " 2614,\n",
       " 2618,\n",
       " ...]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_duplicate_index = data_train[data_train['is_duplicate'] == True].index.to_list()\n",
    "is_duplicate_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicate questions: 111486, number of non duplicate questions 292865\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of duplicate questions: {len(is_duplicate_index)}, number of non duplicate questions {len(data) - len(is_duplicate_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  qid1  qid2                                          question1  \\\n",
       "5    5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "7    7    15    16                     How can I be a good geologist?   \n",
       "11  11    23    24        How do I read and find my YouTube comments?   \n",
       "12  12    25    26               What can make Physics easy to learn?   \n",
       "13  13    27    28        What was your first sexual experience like?   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "5   I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "7           What should I do to be a great geologist?             1  \n",
       "11             How can I see all my Youtube comments?             1  \n",
       "12            How can you make physics easy to learn?             1  \n",
       "13             What was your first sexual experience?             1  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[is_duplicate_index[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting out test and train q1 and q2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train_words = data_train.loc[is_duplicate_index,'question1'].to_numpy()\n",
    "q2_train_words = data_train.loc[is_duplicate_index,'question2'].to_numpy()\n",
    "\n",
    "q1_test_words = data_test['question1'].to_numpy()\n",
    "q2_test_words = data_test['question2'].to_numpy()\n",
    "y_test = data_test['is_duplicate'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAINING QUESTIONS:\\n')\n",
    "print('Question 1: ', q1_train_words[0])\n",
    "print('Question 2: ', q2_train_words[0], '\\n')\n",
    "print('Question 1: ', q1_train_words[5])\n",
    "print('Question 2: ', q2_train_words[5], '\\n')\n",
    "\n",
    "print('TESTING QUESTIONS:\\n')\n",
    "print('Question 1: ', q1_test_words[0])\n",
    "print('Question 2: ', q2_test_words[0], '\\n')\n",
    "print('is_duplicate =', y_test[0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 and Q2 training breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?',\n",
       "        'How can I be a good geologist?',\n",
       "        'How do I read and find my YouTube comments?',\n",
       "        'What can make Physics easy to learn?',\n",
       "        'What was your first sexual experience like?',\n",
       "        'What would a Trump presidency mean for current international master’s students on an F1 visa?',\n",
       "        'What does manipulation mean?',\n",
       "        'Why are so many Quora users posting questions that are readily answered on Google?',\n",
       "        'Why do rockets look white?',\n",
       "        'How should I prepare for CA final law?'], dtype=object),\n",
       " array([\"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\",\n",
       "        'What should I do to be a great geologist?',\n",
       "        'How can I see all my Youtube comments?',\n",
       "        'How can you make physics easy to learn?',\n",
       "        'What was your first sexual experience?',\n",
       "        'How will a Trump presidency affect the students presently in US or planning to study in US?',\n",
       "        'What does manipulation means?',\n",
       "        'Why do people ask Quora questions which can be answered easily by Google?',\n",
       "        'Why are rockets and boosters painted white?',\n",
       "        'How one should know that he/she completely prepare for CA final exam?'],\n",
       "       dtype=object))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_train_words[:10], q2_train_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of q1 train words: 111486, and number of q2 train words 111486\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of q1 train words: {len(q1_train_words)}, and number of q2 train words {len(q2_train_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1 and Q2 testing breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of q1 test words: 10240, number of q2 test words: 10240, number of y test labels: 10240\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of q1 test words: {len(q1_test_words)}, number of q2 test words: {len(q2_test_words)}, number of y test labels: {len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training / validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate questions:111486\n",
      "Length of the training set is: 89188\n",
      "Length of the validation set is: 22298\n"
     ]
    }
   ],
   "source": [
    "cut_off = int(len(q1_train_words) * 0.8)\n",
    "train_q1, train_q2 = q1_train_words[:cut_off], q2_train_words[:cut_off]\n",
    "val_q1, val_q2 = q1_train_words[cut_off:], q1_train_words[cut_off:]\n",
    "print(f\"Number of duplicate questions:{len(q1_train_words)}\")\n",
    "print(f\"Length of the training set is: {len(train_q1)}\")\n",
    "print(f\"Length of the validation set is: {len(val_q1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2 Learning question encoding\n",
    "\n",
    "The next step is to learn how to encode each of the questions as a list of numbers (integers). You will be learning how to encode each word of the selected duplicate pairs with an index. \n",
    "\n",
    "You will start by learning a word dictionary, or vocabulary, containing all the words in your training dataset, which you will use to encode each word of the selected duplicate pairs with an index. \n",
    "\n",
    "For this task you will be using the [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer from Keras. which will take care of everything for you. Begin by setting a seed, so we all get the same encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "text_vectorization = tf.keras.layers.TextVectorization(output_mode=\"int\", split=\"whitespace\", standardize=\"strip_punctuation\")\n",
    "text_vectorization.adapt(np.concatenate((q1_train_words, q2_train_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 36224\n"
     ]
    }
   ],
   "source": [
    "print(f'Vocabulary size: {text_vectorization.vocabulary_size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first question in the train set:\n",
      "\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? \n",
      "\n",
      "encoded version:\n",
      "tf.Tensor(\n",
      "[ 6984     6   178    10  8988  2442 35393   761    13  6636 28205    31\n",
      "    28   483    45    98], shape=(16,), dtype=int64) \n",
      "\n",
      "first question in the test set:\n",
      "\n",
      "How do I prepare for interviews for cse? \n",
      "\n",
      "encoded version:\n",
      "tf.Tensor([    4     8     6   160    17  2079    17 11775], shape=(8,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print('first question in the train set:\\n')\n",
    "print(q1_train_words[0], '\\n') \n",
    "print('encoded version:')\n",
    "print(text_vectorization(q1_train_words[0]),'\\n')\n",
    "\n",
    "print('first question in the test set:\\n')\n",
    "print(q1_test_words[0], '\\n')\n",
    "print('encoded version:')\n",
    "print(text_vectorization(q1_test_words[0]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Defining the Siamese Model\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Understanding Siamese Network \n",
    "A Siamese network is a neural network which uses the same weights while working in tandem on two different input vectors to compute comparable output vectors.The Siamese network you are about to implement looks like this:\n",
    "\n",
    "<img src = \"images/siamese.png\" style=\"width:600px;height:300px;\"/>\n",
    "\n",
    "You get the question embedding, run it through an LSTM layer, normalize $v_1$ and $v_2$, and finally use a triplet loss (explained below) to get the corresponding cosine similarity for each pair of questions. As usual, you will start by importing the data set. The triplet loss makes use of a baseline (anchor) input that is compared to a positive (truthy) input and a negative (falsy) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized. In math equations, you are trying to maximize the following.\n",
    "\n",
    "$$\\mathcal{L}(A, P, N)=\\max \\left(\\|\\mathrm{f}(A)-\\mathrm{f}(P)\\|^{2}-\\|\\mathrm{f}(A)-\\mathrm{f}(N)\\|^{2}+\\alpha, 0\\right)$$\n",
    "\n",
    "$A$ is the anchor input, for example $q1_1$, $P$ the duplicate input, for example, $q2_1$, and $N$ the negative input (the non duplicate question), for example $q2_2$.<br>\n",
    "$\\alpha$ is a margin; you can think about it as a safety net, or by how much you want to push the duplicates from the non duplicates. \n",
    "<br>\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 2 - Siamese\n",
    "\n",
    "**Instructions:** Implement the `Siamese` function below. You should be using all the objects explained below. \n",
    "\n",
    "To implement this model, you will be using `trax`. Concretely, you will be using the following functions.\n",
    "\n",
    "\n",
    "- `tl.Serial`: Combinator that applies layers serially (by function composition) allows you set up the overall structure of the feedforward. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Serial) / [source code](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/combinators.py#L26)\n",
    "    - You can pass in the layers as arguments to `Serial`, separated by commas. \n",
    "    - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))` \n",
    "\n",
    "\n",
    "-  `tl.Embedding`: Maps discrete tokens to vectors. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (also called d_feature) is the number of elements in the word embedding. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding) / [source code](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113)\n",
    "    - `tl.Embedding(vocab_size, d_feature)`.\n",
    "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
    "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example).\n",
    "\n",
    "\n",
    "-  `tl.LSTM` The LSTM layer. It leverages another Trax layer called [`LSTMCell`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTMCell). The number of units should be specified and should match the number of elements in the word embedding. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.rnn.LSTM) / [source code](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/rnn.py#L87)\n",
    "    - `tl.LSTM(n_units)` Builds an LSTM layer of n_units.\n",
    "    \n",
    "    \n",
    "- `tl.Mean`: Computes the mean across a desired axis. Mean uses one tensor axis to form groups of values and replaces each group with the mean value of that group. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean) / [source code](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276)\n",
    "    - `tl.Mean(axis=1)` mean over columns.\n",
    "\n",
    "\n",
    "- `tl.Fn` Layer with no weights that applies the function f, which should be specified using a lambda syntax. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn) / [source doce](https://github.com/google/trax/blob/70f5364dcaf6ec11aabbd918e5f5e4b0f5bfb995/trax/layers/base.py#L576)\n",
    "    - $x$ -> This is used for cosine similarity.\n",
    "    - `tl.Fn('Normalize', lambda x: normalize(x))` Returns a layer with no weights that applies the function `f`\n",
    "    \n",
    "    \n",
    "- `tl.parallel`: It is a combinator layer (like `Serial`) that applies a list of layers in parallel to its inputs. [docs](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.combinators.Parallel) / [source code](https://github.com/google/trax/blob/37aba571a89a8ad86be76a569d0ec4a46bdd8642/trax/layers/combinators.py#L152)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: Siamese\n",
    "def Siamese(text_vectorizer, vocab_size=35224, d_feature=128):\n",
    "    \"\"\"Returns a Siamese model.\n",
    "\n",
    "    Args:\n",
    "        text_vectorizer (TextVectorization): TextVectorization instance, already adapted to your training data.\n",
    "        vocab_size (int, optional): Length of the vocabulary. Defaults to 56400.\n",
    "        d_model (int, optional): Depth of the model. Defaults to 128.\n",
    "        \n",
    "    Returns:\n",
    "        tf.model.Model: A Siamese model. \n",
    "    \n",
    "    \"\"\"\n",
    "    LSTM = tf.keras.models.Sequential(name=\"sequential\")\n",
    "    LSTM.add(text_vectorizer)\n",
    "    LSTM.add(tf.keras.layers.Embedding(name=\"embedding\", input_dim=vocab_size, output_dim=d_feature))\n",
    "    LSTM.add(tf.keras.layers.LSTM(name=\"LSTM\", units=d_feature, return_sequences=True))\n",
    "    LSTM.add(tf.keras.layers.GlobalAveragePooling1D(name=\"mean\"))\n",
    "    LSTM.add(tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1), name='out'))\n",
    "    \n",
    "    input1 = tf.keras.layers.Input((1,), name=\"input_1\", dtype=tf.string)\n",
    "    input2 = tf.keras.layers.Input((1,), name=\"input_2\", dtype=tf.string)\n",
    "    \n",
    "    LSTM1 = LSTM(input1)\n",
    "    LSTM2 = LSTM(input2)\n",
    "    \n",
    "    conc = tf.keras.layers.Concatenate(axis=1, name=\"conc_1_2\")([LSTM1, LSTM2])\n",
    "    return tf.keras.models.Model(inputs=(input1, input2), outputs=conc, name=\"SiameseModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SiameseModel\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 128)                  4768256   ['input_1[0][0]',             \n",
      "                                                                     'input_2[0][0]']             \n",
      "                                                                                                  \n",
      " conc_1_2 (Concatenate)      (None, 256)                  0         ['sequential[0][0]',          \n",
      "                                                                     'sequential[1][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4768256 (18.19 MB)\n",
      "Trainable params: 4768256 (18.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_10 (Tex  (None, None)              0         \n",
      " tVectorization)                                                 \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 128)         4636672   \n",
      "                                                                 \n",
      " LSTM (LSTM)                 (None, None, 128)         131584    \n",
      "                                                                 \n",
      " mean (GlobalAveragePooling  (None, 128)               0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " out (Lambda)                (None, 128)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4768256 (18.19 MB)\n",
      "Trainable params: 4768256 (18.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# check your model\n",
    "model = Siamese(text_vectorization, vocab_size=text_vectorization.vocabulary_size())\n",
    "model.build(input_shape=None)\n",
    "model.summary()\n",
    "model.get_layer(name='sequential').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_dtype=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "w3_unittest.test_Siamese(Siamese)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**  \n",
    "\n",
    "```CPP\n",
    "Parallel_in2_out2[\n",
    "  Serial[\n",
    "    Embedding_41699_128\n",
    "    LSTM_128\n",
    "    Mean\n",
    "    Normalize\n",
    "  ]\n",
    "  Serial[\n",
    "    Embedding_41699_128\n",
    "    LSTM_128\n",
    "    Mean\n",
    "    Normalize\n",
    "  ]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "\n",
    "### 2.2 Hard Negative Mining\n",
    "\n",
    "\n",
    "You will now implement the `TripletLoss` with hard negative mining.<br>\n",
    "As explained in the lecture, you will be using all the questions from each batch to compute this loss. Positive examples are questions $q1_i$, and $q2_i$, while all the other combinations $q1_i$, $q2_j$ ($i\\neq j$), are considered negative examples. The loss will be composed of two terms. One term utilizes the mean of all the non duplicates, the second utilizes the *closest negative*. Our loss expression is then:\n",
    " \n",
    "\\begin{align}\n",
    " \\mathcal{Loss_1(A,P,N)} &=\\max \\left( -cos(A,P)  + mean_{neg} +\\alpha, 0\\right) \\\\\n",
    " \\mathcal{Loss_2(A,P,N)} &=\\max \\left( -cos(A,P)  + closest_{neg} +\\alpha, 0\\right) \\\\\n",
    "\\mathcal{Loss(A,P,N)} &= mean(Loss_1 + Loss_2) \\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Further, two sets of instructions are provided. The first set, found just below, provides a brief description of the task. If that set proves insufficient, a more detailed set can be displayed.  \n",
    "\n",
    "<a name='ex03'></a>\n",
    "### Exercise 02\n",
    "\n",
    "**Instructions (Brief):** Here is a list of things you should do: <br>\n",
    "\n",
    "- As this will be run inside Tensorflow, use all operation supplied by `tf.math` or `tf.linalg`, instead of `numpy` functions. You will also need to explicitly use `tf.shape` to get the batch size from the inputs. This is to make it compatible with the Tensor inputs it will receive when doing actual training and testing. \n",
    "- Use [`tf.linalg.matmul`](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul) to calculate the similarity matrix $v_2v_1^T$ of dimension `batch_size` x `batch_size`. \n",
    "- Take the score of the duplicates on the diagonal with [`tf.linalg.diag_part`](https://www.tensorflow.org/api_docs/python/tf/linalg/diag_part). \n",
    "- Use the `TensorFlow` functions [`tf.eye`](https://www.tensorflow.org/api_docs/python/tf/eye) and [`tf.math.reduce_max`](https://www.tensorflow.org/api_docs/python/tf/math/reduce_max) for the identity matrix and the maximum respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: TripletLossFn\n",
    "def TripletLossFn(v1, v2, margin=0.25):\n",
    "    \"\"\"Custom Loss function.\n",
    "\n",
    "    Args:\n",
    "        v1 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q1.\n",
    "        v2 (numpy.ndarray): Array with dimension (batch_size, model_dimension) associated to Q2.\n",
    "        margin (float, optional): Desired margin. Defaults to 0.25.\n",
    "\n",
    "    Returns:\n",
    "        jax.interpreters.xla.DeviceArray: Triplet Loss.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # use `tf.linalg.matmul` to take the dot product of the two batches. \n",
    "    # Don't forget to transpose the second argument using `transpose_b=True`\n",
    "    #scores = np.dot(np.linalg.norm(v1), np.linalg.norm(v2).T)\n",
    "    \n",
    "    def norm(x):\n",
    "        return tf.math.l2_normalize(x, axis=1)\n",
    "    scores = tf.linalg.matmul(v2,v1, transpose_b=True)\n",
    "    # calculate new batch size\n",
    "    batch_size = tf.cast(tf.shape(scores)[0], scores.dtype)\n",
    "    # use np to grab all postive `diagonal` entries in `scores`\n",
    "    sim_ap = tf.linalg.diag_part(scores)  # the positive ones (duplicates)\n",
    "    # subtract `np.eye(batch_size)` out of 1.0 and do element-wise multiplication with `scores`\n",
    "    sim_an = scores - tf.linalg.diag(sim_ap)\n",
    "    \n",
    "    mean_negative = tf.reduce_sum(sim_an, axis=1) / (batch_size - 1)\n",
    "    # create a composition of two masks: \n",
    "    # the first mask to extract the diagonal elements, \n",
    "    # the second mask to extract elements in the negative_zero_on_duplicate matrix that are larger than the elements in the diagonal \n",
    "    mask1 = (tf.eye(batch_size) == 1)\n",
    "    mask2 = (sim_an > tf.expand_dims(sim_ap, 1))\n",
    "    mask_exclude_positives = tf.cast((mask1 | mask2), scores.dtype)\n",
    "    # multiply `mask_exclude_positives` with 2.0 and subtract it out of `negative_zero_on_duplicate`\n",
    "    negative_without_positive = sim_an - (2.0 * mask_exclude_positives)\n",
    "    # take the row by row `max` of `negative_without_positive`. \n",
    "    # Hint: negative_without_positive.max(axis = [?])  \n",
    "    closest_negative = tf.math.reduce_max(negative_without_positive, axis=1)\n",
    "    # compute `np.maximum` among 0.0 and `A`\n",
    "    # where A = subtract `positive` from `margin` and add `closest_negative`\n",
    "    # IMPORTANT: DO NOT create an extra variable 'A'\n",
    "    triplet_loss1 = tf.maximum(mean_negative - sim_ap + margin, 0.0)\n",
    "    # compute `np.maximum` among 0.0 and `B`\n",
    "    # where B = subtract `positive` from `margin` and add `mean_negative`\n",
    "    # IMPORTANT: DO NOT create an extra variable 'B'\n",
    "    triplet_loss2 = tf.maximum(closest_negative - sim_ap + margin, 0.0)\n",
    "    # add the two losses together and take the `np.sum` of it    \n",
    "    triplet_loss = tf.math.reduce_sum(triplet_loss1 + triplet_loss2)    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return triplet_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: 0.7035076825158911\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([[0.26726124, 0.53452248, 0.80178373],[0.5178918 , 0.57543534, 0.63297887]])\n",
    "v2 = np.array([[ 0.26726124,  0.53452248,  0.80178373],[-0.5178918 , -0.57543534, -0.63297887]])\n",
    "print(\"Triplet Loss:\", TripletLossFn(v1,v2).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TripletLoss(labels, out, margin=0.25):\n",
    "    _, embedding_size = out.shape # get embedding size\n",
    "    v1 = out[:,:int(embedding_size/2)] # Extract v1 from out\n",
    "    v2 = out[:,int(embedding_size/2):] # Extract v2 from out\n",
    "    return TripletLossFn(v1, v2, margin=margin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```CPP\n",
    "Triplet Loss: ~ 0.70\n",
    "```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "w3_unittest.test_TripletLoss(TripletLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "\n",
    "# Part 3: Training\n",
    "\n",
    "Now it's time to finally train your model. As usual, you have to define the cost function and the optimizer. You also have to build the actual model you will be training. \n",
    "\n",
    "To pass the input questions for training and validation you will use the iterator produced by [`tensorflow.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset). Run the next cell to create your train and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(((train_q1, train_q2),tf.constant([1]*len(train_q1))))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((val_q1, val_q2),tf.constant([1]*len(val_q1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=TensorSpec(shape=(89188,), dtype=tf.string, name=None)>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices([train_q1])\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "\n",
    "### 3.1 Training the model\n",
    "\n",
    "You will now write a function that takes in your model to train it. To train your model you have to decide how many times you want to iterate over the entire data set; each iteration is defined as an `epoch`. For each epoch, you have to go over all the data, using your `Dataset` iterator.\n",
    "\n",
    "<a name='ex04'></a>\n",
    "### Exercise 03\n",
    "\n",
    "**Instructions:** Implement the `train_model` below to train the neural network above. Here is a list of things you should do: \n",
    "\n",
    "- Compile the model. Here you will need to pass in:\n",
    "    - `loss=TripletLoss`\n",
    "    - `optimizer=Adam()` with learning rate `lr`\n",
    "- Call the `fit` method. You should pass:\n",
    "    - `train_dataset`\n",
    "    - `epochs`\n",
    "    - `validation_data` \n",
    "\n",
    "\n",
    "\n",
    "You will be using your triplet loss function with Adam optimizer. Also, note that you are not explicitly defining the batch size, because it will be already determined by the `Dataset`.\n",
    "\n",
    "This function will return the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_model\n",
    "def train_model(Siamese, triplet_loss, text_vectorizer, train_dataset, val_dataset, d_feature=128, lr=0.01, train_steps=5):\n",
    "    \"\"\"Training the Siamese Model\n",
    "\n",
    "    Args:\n",
    "        Siamese (function): Function that returns the Siamese model.\n",
    "        TripletLoss (function): Function that defines the TripletLoss loss function.\n",
    "        text_vectorizer: trained instance of `TextVecotrization` \n",
    "        train_dataset (tf.data.Dataset): Training dataset\n",
    "        val_dataset (tf.data.Dataset): Validation dataset\n",
    "        d_feature (int, optional) = size of the encoding. Defaults to 128.\n",
    "        lr (float, optional): learning rate for optimizer. Defaults to 0.01\n",
    "        train_steps (int): number of epochs\n",
    "        \n",
    "    Returns:\n",
    "        tf.keras.Model\n",
    "    \"\"\"\n",
    "    ## START CODE HERE ###\n",
    "\n",
    "    # Instantiate your Siamese model\n",
    "    model = Siamese(text_vectorizer,\n",
    "                    vocab_size = text_vectorization.vocabulary_size(), #set vocab_size accordingly to the size of your vocabulary\n",
    "                    d_feature = d_feature)\n",
    "    # Compile the model\n",
    "    model.compile(loss=triplet_loss,\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "            )\n",
    "    # Train the model \n",
    "    print(f\"train_dataset: {train_dataset}\")\n",
    "    model.fit(train_dataset,\n",
    "              epochs = train_steps,\n",
    "              validation_data = val_dataset,\n",
    "             )\n",
    "             \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: <_BatchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "Epoch 1/2\n",
      "349/349 [==============================] - 176s 433ms/step - loss: 48.7314 - val_loss: 3.8438\n",
      "Epoch 2/2\n",
      "349/349 [==============================] - 137s 388ms/step - loss: 13.6130 - val_loss: 2.9803\n"
     ]
    }
   ],
   "source": [
    "train_steps = 2\n",
    "batch_size = 256\n",
    "train_generator = train_dataset.shuffle(len(train_q1),\n",
    "                                        seed=7, \n",
    "                                        reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "val_generator = val_dataset.shuffle(len(val_q1), \n",
    "                                   seed=7,\n",
    "                                   reshuffle_each_iteration=True).batch(batch_size=batch_size)\n",
    "model = train_model(Siamese, TripletLoss,text_vectorization, \n",
    "                                            train_generator, \n",
    "                                            val_generator, \n",
    "                                            train_steps=train_steps,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset: <_BatchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "train_dataset: <_BatchDataset element_spec=((TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None)), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test your function!\n",
    "w3_unittest.test_train_model(train_model, Siamese, TripletLoss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "\n",
    "# Part 4:  Evaluation  \n",
    "\n",
    "<a name='4.1'></a>\n",
    "\n",
    "### 4.1 Evaluating your siamese network\n",
    "\n",
    "In this section you will learn how to evaluate a Siamese network. You will start by loading a pretrained model, and then you will use it to predict. For the prediction you will need to take the output of your model and compute the cosine loss between each pair of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at model/trained_model.keras",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/mbarbaric/dev/python/NLP/Coursera-NaturalLanguageProcessing/Sequence_Models/week3/assignment.ipynb Cell 45\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mbarbaric/dev/python/NLP/Coursera-NaturalLanguageProcessing/Sequence_Models/week3/assignment.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m'\u001b[39;49m\u001b[39mmodel/trained_model.keras\u001b[39;49m\u001b[39m'\u001b[39;49m, safe_mode\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mbarbaric/dev/python/NLP/Coursera-NaturalLanguageProcessing/Sequence_Models/week3/assignment.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Show the model architecture\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mbarbaric/dev/python/NLP/Coursera-NaturalLanguageProcessing/Sequence_Models/week3/assignment.ipynb#X63sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/dev/python/NLP/Coursera-NaturalLanguageProcessing/Sequence_Models/venv_Sequence_Models/lib/python3.11/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m    263\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39;49mcustom_objects, \u001b[39mcompile\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mcompile\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    264\u001b[0m )\n",
      "File \u001b[0;32m~/dev/python/NLP/Coursera-NaturalLanguageProcessing/Sequence_Models/venv_Sequence_Models/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/dev/python/NLP/Coursera-NaturalLanguageProcessing/Sequence_Models/venv_Sequence_Models/lib/python3.11/site-packages/keras/src/saving/legacy/save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(filepath_str, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 234\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo file or directory found at \u001b[39m\u001b[39m{\u001b[39;00mfilepath_str\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[39mif\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    239\u001b[0m         \u001b[39mreturn\u001b[39;00m saved_model_load\u001b[39m.\u001b[39mload(\n\u001b[1;32m    240\u001b[0m             filepath_str, \u001b[39mcompile\u001b[39m, options\n\u001b[1;32m    241\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at model/trained_model.keras"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('model/trained_model.keras', safe_mode=False, compile=False)\n",
    "\n",
    "# Show the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.2'></a>\n",
    "### 4.2 Classify\n",
    "To determine the accuracy of the model, you will use the test set that was configured earlier. While in training you used only positive examples, the test data, `Q1_test`, `Q2_test` and `y_test`, is set up as pairs of questions, some of which are duplicates and some are not. \n",
    "This routine will run all the test question pairs through the model, compute the cosine similarity of each pair, threshold it and compare the result to `y_test` - the correct response from the data set. The results are accumulated to produce an accuracy; the confusion matrix is also computed to have a better understanding of the errors.\n",
    "\n",
    "\n",
    "<a name='ex05'></a>\n",
    "### Exercise 04\n",
    "\n",
    "**Instructions**  \n",
    " - Loop through the incoming data in batch_size chunks, you will again define a `tensorflow.data.Dataset` to do so. This time you don't need the labels, so you can just replace them by `None`,\n",
    " - compute `v1`, `v2` using the model output,\n",
    " - for each element of the batch\n",
    "        - compute the cosine similarity of each pair of entries, `v1[j]`,`v2[j]`\n",
    "        - determine if `d > threshold`\n",
    "        - increment accuracy if that result matches the expected results (`y_test[j]`)\n",
    "  \n",
    "   Instead of running a for loop, you will vectorize all these operations to make things more efficient,\n",
    " - compute the final accuracy and confusion matrix and return. For the confusion matrix you can use the [`tf.math.confusion_matrix`](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: classify\n",
    "def classify(test_Q1, test_Q2, y_test, threshold, model, batch_size=64, verbose=True):\n",
    "    \"\"\"Function to test the accuracy of the model.\n",
    "\n",
    "    Args:\n",
    "        test_Q1 (numpy.ndarray): Array of Q1 questions. Each element of the array would be a string.\n",
    "        test_Q2 (numpy.ndarray): Array of Q2 questions. Each element of the array would be a string.\n",
    "        y_test (numpy.ndarray): Array of actual target.\n",
    "        threshold (float): Desired threshold\n",
    "        model (tensorflow.Keras.Model): The Siamese model.\n",
    "        batch_size (int, optional): Size of the batches. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model\n",
    "        numpy.array: confusion matrix\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    test_gen = tf.data.Dataset.from_tensor_slices(((test_Q1, test_Q2),None)).batch(batch_size=batch_size)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    pred = None\n",
    "    _, n_feat = None\n",
    "    v1 = None\n",
    "    v2 = None\n",
    "    \n",
    "    # Compute the cosine similarity. Using `tf.math.reduce_sum`. \n",
    "    # Don't forget to use the appropriate axis argument.\n",
    "    d  = None\n",
    "    # Check if d>threshold to make predictions\n",
    "    y_pred = tf.cast(None, tf.float64)\n",
    "    # take the average of correct predictions to get the accuracy\n",
    "    accuracy = None\n",
    "    # compute the confusion matrix using `tf.math.confusion_matrix`\n",
    "    cm = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return accuracy, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes around 1 minute\n",
    "accuracy, cm = classify(q1_test,q2_test, y_test, 0.7, model,  batch_size = 512) \n",
    "print(\"Accuracy\", accuracy.numpy())\n",
    "print(f\"Confusion matrix:\\n{cm.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Expected Result**  \n",
    "Accuracy ~0.725\n",
    "\n",
    "Confusion matrix:\n",
    "```\n",
    "[[4876 1506]\n",
    " [1300 2558]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function!\n",
    "w3_unittest.test_classify(classify, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "\n",
    "# Part 5: Testing with your own questions\n",
    "\n",
    "In this final section you will test the model with your own questions. You will write a function `predict` which takes two questions as input and returns `True` or `False` depending on whether the question pair is a duplicate or not.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `predict` that takes in two questions, the threshold and the model, and returns whether the questions are duplicates (`True`) or not duplicates (`False`) given a similarity threshold. \n",
    "\n",
    "<a name='ex06'></a>\n",
    "### Exercise 05\n",
    "\n",
    "\n",
    "**Instructions:** \n",
    "- Create a tensorflow.data.Dataset from your two questions. Again, labels are not important, so you simply write `None`\n",
    "- use the trained model output to create `v1`, `v2`\n",
    "- compute the cosine similarity (dot product) of `v1`, `v2`\n",
    "- compute `res` by comparing d to the threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "def predict(question1, question2, threshold, model, verbose=False):\n",
    "    \"\"\"Function for predicting if two questions are duplicates.\n",
    "\n",
    "    Args:\n",
    "        question1 (str): First question.\n",
    "        question2 (str): Second question.\n",
    "        threshold (float): Desired threshold.\n",
    "        model (tensorflow.keras.Model): The Siamese model.\n",
    "        data_generator (function): Data generator function. Defaults to data_generator.\n",
    "        verbose (bool, optional): If the results should be printed out. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the questions are duplicates, False otherwise.\n",
    "    \"\"\"\n",
    "    generator = tf.data.Dataset.from_tensor_slices((([question1], [question2]),None)).batch(batch_size=1)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Call the predict method of your model and save the output into v1v2\n",
    "    v1v2 = None\n",
    "    # Extract v1 and v2 from the model output\n",
    "    v1 = None\n",
    "    v2 = None\n",
    "    # Take the dot product to compute cos similarity of each pair of entries, v1, v2\n",
    "    # Since v1 and v2 are both vectors, use the function tf.math.reduce_sum instead of tf.linalg.matmul\n",
    "    d = None\n",
    "    # Is d greater than the threshold?\n",
    "    res = None\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if(verbose):\n",
    "        print(\"Q1  = \", question1, \"\\nQ2  = \", question2)\n",
    "        print(\"d   = \", d.numpy())\n",
    "        print(\"res = \", res.numpy())\n",
    "\n",
    "    return res.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try with your own questions\n",
    "question1 = \"When will I see you?\"\n",
    "question2 = \"When can I see you again?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected Output\n",
    "If input is:\n",
    "```\n",
    "question1 = \"When will I see you?\"\n",
    "question2 = \"When can I see you again?\"\n",
    "```\n",
    "\n",
    "Output is (d may vary a bit):\n",
    "```\n",
    "1/1 [==============================] - 0s 13ms/step\n",
    "Q1  =  When will I see you? \n",
    "Q2  =  When can I see you again?\n",
    "d   =  0.8422112\n",
    "res =  True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to try with your own questions\n",
    "question1 = \"Do they enjoy eating the dessert?\"\n",
    "question2 = \"Do they like hiking in the desert?\"\n",
    "# 1 means it is duplicated, 0 otherwise\n",
    "predict(question1 , question2, 0.7, model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected output\n",
    "\n",
    "If input is:\n",
    "```\n",
    "question1 = \"Do they enjoy eating the dessert?\"\n",
    "question2 = \"Do they like hiking in the desert?\"\n",
    "```\n",
    "\n",
    "Output (d may vary a bit):\n",
    "\n",
    "```\n",
    "1/1 [==============================] - 0s 12ms/step\n",
    "Q1  =  Do they enjoy eating the dessert? \n",
    "Q2  =  Do they like hiking in the desert?\n",
    "d   =  0.12625802\n",
    "res =  False\n",
    "\n",
    "False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function!\n",
    "w3_unittest.test_predict(predict, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
