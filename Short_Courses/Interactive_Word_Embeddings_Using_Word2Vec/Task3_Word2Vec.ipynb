{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Recommendation System with Word Embeddings using Word2Vec, Plotly, and NetworkX\n",
    "\n",
    "## Project Breakdown\n",
    "- Task 1: Introduction\n",
    "- Task 2: Exploratory Data Analysis and Preprocessing\n",
    "- Task 3: Word2Vec with Gensim (you are here)\n",
    "- Task 4: Exploring Results\n",
    "- Task 5: Building and Visualizing Interactive Network Graph\n",
    "\n",
    "## Task 3: Word2Vec with Gensim\n",
    "Word2Vec original papers can be found [here](https://arxiv.org/pdf/1301.3781.pdf) and [here](https://arxiv.org/pdf/1310.4546.pdf), while the documentation for the Gensim model can be found [here](https://radimrehurek.com/gensim/models/word2vec.html).\n",
    "\n",
    "![Word2Vec architecture](Data/word2vec.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['place',\n",
       " 'the',\n",
       " 'chicken',\n",
       " 'butter',\n",
       " 'soup',\n",
       " 'and',\n",
       " 'onion',\n",
       " 'in',\n",
       " 'a',\n",
       " 'slow',\n",
       " 'cooker',\n",
       " 'and',\n",
       " 'fill',\n",
       " 'with',\n",
       " 'enough',\n",
       " 'water',\n",
       " 'to',\n",
       " 'cover',\n",
       " 'cover',\n",
       " 'and',\n",
       " 'cook',\n",
       " 'for',\n",
       " 'to',\n",
       " 'hours',\n",
       " 'on',\n",
       " 'high',\n",
       " 'about',\n",
       " 'minutes',\n",
       " 'before',\n",
       " 'serving',\n",
       " 'place',\n",
       " 'the',\n",
       " 'torn',\n",
       " 'biscuit',\n",
       " 'dough',\n",
       " 'in',\n",
       " 'the',\n",
       " 'slow',\n",
       " 'cooker',\n",
       " 'cook',\n",
       " 'until',\n",
       " 'the',\n",
       " 'dough',\n",
       " 'is',\n",
       " 'no',\n",
       " 'longer',\n",
       " 'raw',\n",
       " 'in',\n",
       " 'the',\n",
       " 'center']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('Data/train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    \n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m            Word2Vec\n",
      "\u001b[0;31mString form:\u001b[0m     Word2Vec<vocab=0, vector_size=100, alpha=0.025>\n",
      "\u001b[0;31mFile:\u001b[0m            ~/dev/ai_projects/NLP/.venv/lib/python3.10/site-packages/gensim/models/word2vec.py\n",
      "\u001b[0;31mDocstring:\u001b[0m       <no docstring>\n",
      "\u001b[0;31mClass docstring:\u001b[0m\n",
      "Serialize/deserialize objects from disk, by equipping them with the `save()` / `load()` methods.\n",
      "\n",
      "Warnings\n",
      "--------\n",
      "This uses pickle internally (among other techniques), so objects must not contain unpicklable attributes\n",
      "such as lambda functions etc.\n",
      "\u001b[0;31mInit docstring:\u001b[0m \n",
      "Train, use and evaluate neural networks described in https://code.google.com/p/word2vec/.\n",
      "\n",
      "Once you're finished training a model (=no more updates, only querying)\n",
      "store and use only the :class:`~gensim.models.keyedvectors.KeyedVectors` instance in ``self.wv``\n",
      "to reduce memory.\n",
      "\n",
      "The full model can be stored/loaded via its :meth:`~gensim.models.word2vec.Word2Vec.save` and\n",
      ":meth:`~gensim.models.word2vec.Word2Vec.load` methods.\n",
      "\n",
      "The trained word vectors can also be stored/loaded from a format compatible with the\n",
      "original word2vec implementation via `self.wv.save_word2vec_format`\n",
      "and :meth:`gensim.models.keyedvectors.KeyedVectors.load_word2vec_format`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "sentences : iterable of iterables, optional\n",
      "    The `sentences` iterable can be simply a list of lists of tokens, but for larger corpora,\n",
      "    consider an iterable that streams the sentences directly from disk/network.\n",
      "    See :class:`~gensim.models.word2vec.BrownCorpus`, :class:`~gensim.models.word2vec.Text8Corpus`\n",
      "    or :class:`~gensim.models.word2vec.LineSentence` in :mod:`~gensim.models.word2vec` module for such examples.\n",
      "    See also the `tutorial on data streaming in Python\n",
      "    <https://rare-technologies.com/data-streaming-in-python-generators-iterators-iterables/>`_.\n",
      "    If you don't supply `sentences`, the model is left uninitialized -- use if you plan to initialize it\n",
      "    in some other way.\n",
      "corpus_file : str, optional\n",
      "    Path to a corpus file in :class:`~gensim.models.word2vec.LineSentence` format.\n",
      "    You may use this argument instead of `sentences` to get performance boost. Only one of `sentences` or\n",
      "    `corpus_file` arguments need to be passed (or none of them, in that case, the model is left uninitialized).\n",
      "vector_size : int, optional\n",
      "    Dimensionality of the word vectors.\n",
      "window : int, optional\n",
      "    Maximum distance between the current and predicted word within a sentence.\n",
      "min_count : int, optional\n",
      "    Ignores all words with total frequency lower than this.\n",
      "workers : int, optional\n",
      "    Use these many worker threads to train the model (=faster training with multicore machines).\n",
      "sg : {0, 1}, optional\n",
      "    Training algorithm: 1 for skip-gram; otherwise CBOW.\n",
      "hs : {0, 1}, optional\n",
      "    If 1, hierarchical softmax will be used for model training.\n",
      "    If 0, and `negative` is non-zero, negative sampling will be used.\n",
      "negative : int, optional\n",
      "    If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\"\n",
      "    should be drawn (usually between 5-20).\n",
      "    If set to 0, no negative sampling is used.\n",
      "ns_exponent : float, optional\n",
      "    The exponent used to shape the negative sampling distribution. A value of 1.0 samples exactly in proportion\n",
      "    to the frequencies, 0.0 samples all words equally, while a negative value samples low-frequency words more\n",
      "    than high-frequency words. The popular default value of 0.75 was chosen by the original Word2Vec paper.\n",
      "    More recently, in https://arxiv.org/abs/1804.04212, Caselles-Dupr√©, Lesaint, & Royo-Letelier suggest that\n",
      "    other values may perform better for recommendation applications.\n",
      "cbow_mean : {0, 1}, optional\n",
      "    If 0, use the sum of the context word vectors. If 1, use the mean, only applies when cbow is used.\n",
      "alpha : float, optional\n",
      "    The initial learning rate.\n",
      "min_alpha : float, optional\n",
      "    Learning rate will linearly drop to `min_alpha` as training progresses.\n",
      "seed : int, optional\n",
      "    Seed for the random number generator. Initial vectors for each word are seeded with a hash of\n",
      "    the concatenation of word + `str(seed)`. Note that for a fully deterministically-reproducible run,\n",
      "    you must also limit the model to a single worker thread (`workers=1`), to eliminate ordering jitter\n",
      "    from OS thread scheduling. (In Python 3, reproducibility between interpreter launches also requires\n",
      "    use of the `PYTHONHASHSEED` environment variable to control hash randomization).\n",
      "max_vocab_size : int, optional\n",
      "    Limits the RAM during vocabulary building; if there are more unique\n",
      "    words than this, then prune the infrequent ones. Every 10 million word types need about 1GB of RAM.\n",
      "    Set to `None` for no limit.\n",
      "max_final_vocab : int, optional\n",
      "    Limits the vocab to a target vocab size by automatically picking a matching min_count. If the specified\n",
      "    min_count is more than the calculated min_count, the specified min_count will be used.\n",
      "    Set to `None` if not required.\n",
      "sample : float, optional\n",
      "    The threshold for configuring which higher-frequency words are randomly downsampled,\n",
      "    useful range is (0, 1e-5).\n",
      "hashfxn : function, optional\n",
      "    Hash function to use to randomly initialize weights, for increased training reproducibility.\n",
      "epochs : int, optional\n",
      "    Number of iterations (epochs) over the corpus. (Formerly: `iter`)\n",
      "trim_rule : function, optional\n",
      "    Vocabulary trimming rule, specifies whether certain words should remain in the vocabulary,\n",
      "    be trimmed away, or handled using the default (discard if word count < min_count).\n",
      "    Can be None (min_count will be used, look to :func:`~gensim.utils.keep_vocab_item`),\n",
      "    or a callable that accepts parameters (word, count, min_count) and returns either\n",
      "    :attr:`gensim.utils.RULE_DISCARD`, :attr:`gensim.utils.RULE_KEEP` or :attr:`gensim.utils.RULE_DEFAULT`.\n",
      "    The rule, if given, is only used to prune vocabulary during build_vocab() and is not stored as part of the\n",
      "    model.\n",
      "\n",
      "    The input parameters are of the following types:\n",
      "        * `word` (str) - the word we are examining\n",
      "        * `count` (int) - the word's frequency count in the corpus\n",
      "        * `min_count` (int) - the minimum count threshold.\n",
      "sorted_vocab : {0, 1}, optional\n",
      "    If 1, sort the vocabulary by descending frequency before assigning word indexes.\n",
      "    See :meth:`~gensim.models.keyedvectors.KeyedVectors.sort_by_descending_frequency()`.\n",
      "batch_words : int, optional\n",
      "    Target size (in words) for batches of examples passed to worker threads (and\n",
      "    thus cython routines).(Larger batches will be passed if individual\n",
      "    texts are longer than 10000 words, but the standard cython code truncates to that maximum.)\n",
      "compute_loss: bool, optional\n",
      "    If True, computes and stores loss value which can be retrieved using\n",
      "    :meth:`~gensim.models.word2vec.Word2Vec.get_latest_training_loss`.\n",
      "callbacks : iterable of :class:`~gensim.models.callbacks.CallbackAny2Vec`, optional\n",
      "    Sequence of callbacks to be executed at specific stages during training.\n",
      "shrink_windows : bool, optional\n",
      "    New in 4.1. Experimental.\n",
      "    If True, the effective window size is uniformly sampled from  [1, `window`]\n",
      "    for each target word during training, to match the original word2vec algorithm's\n",
      "    approximate weighting of context words by distance. Otherwise, the effective\n",
      "    window size is always fixed to `window` words to either side.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Initialize and train a :class:`~gensim.models.word2vec.Word2Vec` model\n",
      "\n",
      ".. sourcecode:: pycon\n",
      "\n",
      "    >>> from gensim.models import Word2Vec\n",
      "    >>> sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
      "    >>> model = Word2Vec(sentences, min_count=1)\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "wv : :class:`~gensim.models.keyedvectors.KeyedVectors`\n",
      "    This object essentially contains the mapping between words and embeddings. After training, it can be used\n",
      "    directly to query those embeddings in various ways. See the module level docstring for examples."
     ]
    }
   ],
   "source": [
    "model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 15s, sys: 3.12 s, total: 4min 18s\n",
      "Wall time: 1min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(92907521, 125366090)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dressing', 0.7689827680587769),\n",
       " ('mesclun', 0.7343204021453857),\n",
       " ('vinaigrette', 0.7328872680664062),\n",
       " ('slaw', 0.7293946146965027),\n",
       " ('mache', 0.683726966381073),\n",
       " ('lettuces', 0.6635078191757202),\n",
       " ('mizuna', 0.6586458683013916),\n",
       " ('fris√©e', 0.6550888419151306),\n",
       " ('frisee', 0.6550101637840271),\n",
       " ('salads', 0.6481441259384155),\n",
       " ('thousand', 0.645386278629303),\n",
       " ('tabbouleh', 0.6379859447479248),\n",
       " ('dressed', 0.6374696493148804),\n",
       " ('caesar', 0.6362305283546448),\n",
       " ('micro', 0.6318367719650269),\n",
       " ('tartare', 0.6228469610214233),\n",
       " ('spago', 0.6170496344566345),\n",
       " ('m√¢che', 0.6120429039001465),\n",
       " ('zesty', 0.609458327293396),\n",
       " ('island', 0.6069187521934509)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('salad', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Data/w2v.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
