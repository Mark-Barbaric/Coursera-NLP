{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Siamese model using Trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow import math\n",
    "import numpy\n",
    "\n",
    "numpy.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor is of type: <class 'numpy.ndarray'>\n",
      "\n",
      "And looks like this:\n",
      "\n",
      " [[0.77132064 0.02075195 0.63364823 0.74880388 0.49850701]\n",
      " [0.22479665 0.19806286 0.76053071 0.16911084 0.08833981]]\n"
     ]
    }
   ],
   "source": [
    "tensor = numpy.random.random((2, 5))\n",
    "print(f'The tensor is of type: {type(tensor)}\\n\\nAnd looks like this:\\n\\n {tensor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a `Siamese` model you will first need to create a LSTM model. For this you can stack layers using the`Sequential` model. To retrieve the output of both branches of the Siamese model, you can concatenate results using the `Concatenate` layer. You should be familiar with the following layers (notice each layer can be clicked to go to the docs):\n",
    "   - [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) groups a linear stack of layers into a [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n",
    "   - [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) Maps positive integers into vectors of fixed size. It will have shape (vocabulary length X dimension of output vectors). The dimension of output vectors (called `model_dimension`in the code) is the number of elements in the word embedding. \n",
    "   - [`LSTM`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) The Long Short-Term Memory (LSTM) layer. The number of units should be specified and should match the number of elements in the word embedding. \n",
    "   - [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) Computes global average pooling, which essentially takes the mean across a desired axis. GlobalAveragePooling1D uses one tensor axis to form groups of values and replaces each group with the mean value of that group. \n",
    "   - [`Lambda`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.base.Fn)  Layer with no weights that applies the function f, which should be specified using a lambda syntax. You will use this layer to apply normalization with the function\n",
    "        - `tfmath.l2_normalize(x)`\n",
    "        \n",
    "- [`Concatenate`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate) Layer that concatenates a list of inputs. This layer will concatenate the normalized outputs of each LSTM into a single output for the model.\n",
    "- [`Input`](https://www.tensorflow.org/api_docs/python/tf/keras/Input): it is used to instantiate a Keras tensor.. Remember to set correctly the dimension and type of the input, which are batches of questions. \n",
    "\n",
    "\n",
    "Putting everything together the Siamese model will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 500\n",
    "model_dimension = 128\n",
    "\n",
    "# Define the LSTM model\n",
    "LSTM = Sequential()\n",
    "LSTM.add(layers.Embedding(input_dim=vocab_size, output_dim=model_dimension))\n",
    "LSTM.add(layers.LSTM(units=model_dimension, return_sequences = True))\n",
    "LSTM.add(layers.AveragePooling1D())\n",
    "LSTM.add(layers.Lambda(lambda x: math.l2_normalize(x)))\n",
    "\n",
    "input1 = layers.Input((vocab_size,))\n",
    "input2 = layers.Input((vocab_size,))\n",
    "\n",
    "# Concatenate two LSTMs together\n",
    "conc = layers.Concatenate(axis=1)((LSTM(input1), LSTM(input2)))\n",
    "    \n",
    "\n",
    "# Use the Parallel combinator to create a Siamese model out of the LSTM \n",
    "Siamese = Model(inputs=(input1, input2), outputs=conc)\n",
    "\n",
    "# Print the summary of the model\n",
    "Siamese.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siamese model:\n",
      "\n",
      "Total layers: 2\n",
      "\n",
      "========\n",
      "Parallel.sublayers_0: Serial[\n",
      "  Embedding_500_128\n",
      "  LSTM_128\n",
      "  Mean\n",
      "  Normalize\n",
      "]\n",
      "\n",
      "========\n",
      "Parallel.sublayers_1: Serial[\n",
      "  Embedding_500_128\n",
      "  LSTM_128\n",
      "  Mean\n",
      "  Normalize\n",
      "]\n",
      "\n",
      "Detail of LSTM models:\n",
      "\n",
      "Total layers: 4\n",
      "\n",
      "========\n",
      "Serial.sublayers_0: Embedding_500_128\n",
      "\n",
      "========\n",
      "Serial.sublayers_1: LSTM_128\n",
      "\n",
      "========\n",
      "Serial.sublayers_2: Mean\n",
      "\n",
      "========\n",
      "Serial.sublayers_3: Normalize\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_layers(model, layer_prefix):\n",
    "    print(f\"Total layers: {len(model.layers)}\\n\")\n",
    "    for i in range(len(model.layers)):\n",
    "        print('========')\n",
    "        print(f'{layer_prefix}_{i}: {model.layers[i]}\\n')\n",
    "\n",
    "print('Siamese model:\\n')\n",
    "show_layers(Siamese, 'Parallel.sublayers')\n",
    "\n",
    "print('Detail of LSTM models:\\n')\n",
    "show_layers(LSTM, 'Serial.sublayers')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
