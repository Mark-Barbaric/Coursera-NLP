{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Three Ways of Attention and Dot Product Attention: Ungraded Lab Notebook\n",
    "\n",
    "In this notebook you'll explore the three ways of attention (encoder-decoder attention, causal attention, and bi-directional self attention) and how to implement the latter two with dot product attention. \n",
    "\n",
    "## Background\n",
    "\n",
    "As you learned last week, **attention models** constitute powerful tools in the NLP practitioner's toolkit. Like LSTMs, they learn which words are most important to phrases, sentences, paragraphs, and so on. Moreover, they mitigate the vanishing gradient problem even better than LSTMs. You've already seen how to combine attention with LSTMs to build **encoder-decoder models** for applications such as machine translation. \n",
    "\n",
    "<img src=\"../images/C4_W2_L3_dot-product-attention_S01_introducing-attention_stripped.png\" width=\"500\"/>\n",
    "\n",
    "This week, you'll see how to integrate attention into **transformers**. Because transformers do not process one token at a time, they are much easier to parallelize and accelerate. Beyond text summarization, applications of transformers include: \n",
    "* Machine translation\n",
    "* Auto-completion\n",
    "* Named Entity Recognition\n",
    "* Chatbots\n",
    "* Question-Answering\n",
    "* And more!\n",
    "\n",
    "Along with embedding, positional encoding, dense layers, and residual connections, attention is a crucial component of transformers. At the heart of any attention scheme used in a transformer is **dot product attention**, of which the figures below display a simplified picture:\n",
    "\n",
    "<img src=\"../images/C4_W2_L3_dot-product-attention_S03_concept-of-attention_stripped.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"../images/C4_W2_L3_dot-product-attention_S04_attention-math_stripped.png\" width=\"500\"/>\n",
    "\n",
    "With basic dot product attention, you capture the interactions between every word (embedding) in your query and every word in your key. If the queries and keys belong to the same sentences, this constitutes **bi-directional self-attention**. In some situations, however, it's more appropriate to consider only words which have come before the current one. Such cases, particularly when the queries and keys come from the same sentences, fall into the category of **causal attention**. \n",
    "\n",
    "<img src=\"../images/C4_W2_L4_causal-attention_S02_causal-attention_stripped.png\" width=\"500\"/>\n",
    "\n",
    "For causal attention, you add a **mask** to the argument of our softmax function, as illustrated below: \n",
    "\n",
    "<img src=\"../images/C4_W2_L4_causal-attention_S03_causal-attention-math_stripped.png\" width=\"500\"/>\n",
    "\n",
    "<img src=\"../images/C4_W2_L4_causal-attention_S04_causal-attention-math-2_stripped.png\" width=\"500\"/>\n",
    "\n",
    "Now let's see how to implement the attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 14:55:25.924842: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-25 14:55:27.220109: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-25 14:55:27.220298: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-25 14:55:27.497008: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-25 14:55:27.882077: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-25 14:55:27.885671: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-25 14:55:32.512670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import sys\n",
    "import textwrap\n",
    "wrapper = textwrap.TextWrapper(width=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tensor(t, name):\n",
    "    \"\"\"Display shape and tensor\"\"\"\n",
    "    print(f'{name} shape: {t.shape}\\n')\n",
    "    print(f'{t}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tensors for key, value and query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = tf.constant([[1.0, 0.0, 3.0], [0.0, 1.0, 0.0]])\n",
    "k = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "v = tf.constant([[0.0, 1.0, 1.5], [3.0, 4.0, 5.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query shape: (2, 3)\n",
      "\n",
      "[[1. 0. 3.]\n",
      " [0. 1. 0.]]\n",
      "\n",
      "key shape: (2, 3)\n",
      "\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n",
      "value shape: (2, 3)\n",
      "\n",
      "[[0.  1.  1.5]\n",
      " [3.  4.  5. ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_tensor(q, 'query')\n",
    "display_tensor(k, 'key')\n",
    "display_tensor(v, 'value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_test = tf.constant([[1.0, 2.0, 3.0], [0.0, 2.0, 3.2]])\n",
    "mask = tf.constant([[0.0, 1.0, 0.0], [0.0, 0.0, 0.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[1., 0., 0.],\n",
       "       [1., 1., 0.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_2 = tf.experimental.numpy.tril(tf.ones((2, 2)))  \n",
    "mask_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[-1.e+09,  2.e+00, -1.e+09],\n",
       "       [-1.e+09, -1.e+09, -1.e+09]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = q_test + (1. - mask) * -1e9\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product attention\n",
    "\n",
    "Here you compute \n",
    "$\\textrm{softmax} \\left(\\frac{Q K^T}{\\sqrt{d}} + M \\right) V$, where the (optional, but default) scaling factor $\\sqrt{d}$ is the square root of the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product_attention(q, k, v, mask, scale=True):\n",
    "    \"\"\"\n",
    "    Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "    Arguments:\n",
    "        q (tf.Tensor): query of shape (..., seq_len_q, depth)\n",
    "        k (tf.Tensor): key of shape (..., seq_len_k, depth)\n",
    "        v (tf.Tensor): value of shape (..., seq_len_v, depth_v)\n",
    "        mask (tf.Tensor): mask with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "        scale (boolean): if True, the result is a scaled dot-product attention. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        attention_output (tf.Tensor): the result of the attention function\n",
    "    \"\"\"\n",
    "    nominator = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    if scale:\n",
    "        dk = tf.cast(q.shape[-1], tf.float32)\n",
    "        nominator /= tf.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_coursera_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
