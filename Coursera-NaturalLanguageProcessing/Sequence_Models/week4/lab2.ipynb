{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Modified Triplet Loss : Ungraded Lecture Notebook\n",
    "In this notebook you'll see how to calculate the full triplet loss, step by step, including the mean negative and the closest negative. You'll also calculate the matrix of similarity scores.\n",
    "\n",
    "## Background\n",
    "This is the original triplet loss function:\n",
    "\n",
    "$\\mathcal{L_\\mathrm{Original}} = \\max{(\\mathrm{s}(A,N) -\\mathrm{s}(A,P) +\\alpha, 0)}$\n",
    "\n",
    "It can be improved by including the mean negative and the closest negative, to create a new full loss function. The inputs are the Anchor $\\mathrm{A}$, Positive $\\mathrm{P}$ and Negative $\\mathrm{N}$.\n",
    "\n",
    "$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
    "\n",
    "$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
    "\n",
    "$\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}$\n",
    "\n",
    "Let me show you what that means exactly, and how to calculate each step.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Scores\n",
    "The first step is to calculate the matrix of similarity scores using cosine similarity so that you can look up $\\mathrm{s}(A,P)$, $\\mathrm{s}(A,N)$ as needed for the loss formulas.\n",
    "\n",
    "### Two Vectors\n",
    "First I'll show you how to calculate the similarity score, using cosine similarity, for 2 vectors.\n",
    "\n",
    "$\\mathrm{s}(v_1,v_2) = \\mathrm{cosine \\ similarity}(v_1,v_2) = \\frac{v_1 \\cdot v_2}{||v_1||~||v_2||}$\n",
    "* Try changing the values in the second vector to see how it changes the cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(l1 : np.array, l2 : np.array):\n",
    "    numerator = np.dot(l1, l2)\n",
    "    denominator = np.sqrt(np.dot(l1, l1)) * np.sqrt(np.dot(l2, l2))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity: 0.9974086507360697\n"
     ]
    }
   ],
   "source": [
    "v1 = np.array([1,2,3], dtype=float)\n",
    "v2 = np.array([1,2,3.5])\n",
    "\n",
    "print(f\"cosine_similarity: {cosine_similarity(v1, v2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Batches of Vectors\n",
    "Now i'll show you how to calculate the similarity scores, using cosine similarity, for 2 batches of vectors. These are rows of individual vectors, just like in the example above, but stacked vertically into a matrix. They would look like the image below for a batch size (row count) of 4 and embedding size (column count) of 5.\n",
    "\n",
    "The data is setup so that $v_{1\\_1}$ and $v_{2\\_1}$ represent duplicate inputs, but they are not duplicates with any other rows in the batch. This means $v_{1\\_1}$ and $v_{2\\_1}$ (green and green) have more similar vectors than say $v_{1\\_1}$ and $v_{2\\_2}$ (green and magenta).\n",
    "\n",
    "I'll show you two different methods for calculating the matrix of similarities from 2 batches of vectors.\n",
    "\n",
    "<img src = 'images/v1v2_stacked.png' width=\"width\" height=\"height\" style=\"height:250px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup vector batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1_1 = np.array([1, 2, 3])\n",
    "v1_2 = np.array([9, 8, 7])\n",
    "v1_3 = np.array([-1, -4, -2])\n",
    "v1_4 = np.array([1, -7, 2])\n",
    "\n",
    "v1 = np.stack([v1_1, v1_2, v1_3, v1_4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2_1 = v1_1 + np.random.normal(0, 2, 3)\n",
    "v2_2 = v1_2 + np.random.normal(0, 2, 3)\n",
    "v2_3 = v1_3 + np.random.normal(0, 2, 3)\n",
    "v2_4 = v1_4 + np.random.normal(0, 2, 3)\n",
    "\n",
    "v2 = np.stack([v2_1, v2_2, v2_3, v2_4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Score Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_scores_1(m1 : np.array, m2 : np.array):\n",
    "    assert m1.shape == m2.shape\n",
    "    sim = np.zeros([len(m1), len(m2)])\n",
    "    \n",
    "    for row in range(m1.shape[0]):\n",
    "        for col in range(m2.shape[1]):\n",
    "            sim[row][col] = cosine_similarity(m1[row], m2[col])\n",
    "    \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_scores_2(m1 : np.array, m2 : np.array):\n",
    "    assert m1.shape == m2.shape\n",
    "    \n",
    "    def norm(x : np.array):\n",
    "        return x / np.sqrt(np.sum(x * x, axis = 1, keepdims=True))\n",
    "\n",
    "    return np.dot(norm(m1), norm(m2).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]),\n",
       " array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_test = np.ones(shape=(3,3))\n",
    "v2_test = np.ones(shape=(3,3))\n",
    "sim_scores_1 = similarity_scores_1(v1_test, v2_test)\n",
    "sim_scores_2 = similarity_scores_2(v1_test, v2_test)\n",
    "\n",
    "sim_scores_1, sim_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.94280904, 0.94280904, 0.94280904],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ]]),\n",
       " array([[0.94280904, 0.94280904, 0.94280904],\n",
       "        [1.        , 1.        , 1.        ],\n",
       "        [1.        , 1.        , 1.        ]]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_test[0][1]=2\n",
    "\n",
    "sim_scores_1 = similarity_scores_1(v1_test, v2_test)\n",
    "sim_scores_2 = similarity_scores_2(v1_test, v2_test)\n",
    "sim_scores_1, sim_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.75414925,  0.80763641, -0.58169271,  0.        ],\n",
       "        [ 0.37620945,  0.98723393, -0.43778957,  0.        ],\n",
       "        [-0.44085198, -0.85491104,  0.83337639,  0.        ],\n",
       "        [ 0.1514151 , -0.40580418,  0.86766685,  0.        ]]),\n",
       " array([[ 0.75414925,  0.80763641, -0.58169271,  0.46182764],\n",
       "        [ 0.37620945,  0.98723393, -0.43778957,  0.34460704],\n",
       "        [-0.44085198, -0.85491104,  0.83337639,  0.01207406],\n",
       "        [ 0.1514151 , -0.40580418,  0.86766685,  0.73836405]]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_scores_1 = similarity_scores_1(v1, v2)\n",
    "sim_scores_2 = similarity_scores_2(v1, v2)\n",
    "sim_scores_1, sim_scores_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Negative Mining\n",
    "\n",
    "I'll now show you how to calculate the mean negative $mean\\_neg$ and the closest negative $close\\_neg$ used in calculating $\\mathcal{L_\\mathrm{1}}$ and $\\mathcal{L_\\mathrm{2}}$.\n",
    "\n",
    "\n",
    "$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
    "\n",
    "$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
    "\n",
    "You'll do this using the matrix of similarity scores you already know how to make, like the example below for a batch size of 4. The diagonal of the matrix contains all the $\\mathrm{s}(A,P)$ values, similarities from duplicate question pairs (aka Positives). This is an important attribute for the calculations to follow.\n",
    "\n",
    "<img src = 'images/ss_matrix.png' width=\"width\" height=\"height\" style=\"height:250px;\"/>\n",
    "\n",
    "\n",
    "### Mean Negative\n",
    "$mean\\_neg$ is the average of the off diagonals, the $\\mathrm{s}(A,N)$ values, for each row.\n",
    "\n",
    "### Closest Negative\n",
    "$closest\\_neg$ is the largest off diagonal value, $\\mathrm{s}(A,N)$, that is smaller than the diagonal $\\mathrm{s}(A,P)$ for each row.\n",
    "* Try using a different matrix of similarity scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9, -0.8,  0.3, -0.5],\n",
       "       [-0.4,  0.5,  0.1, -0.1],\n",
       "       [ 0.3,  0.1, -0.4, -0.8],\n",
       "       [-0.5, -0.2, -0.7,  0.5]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_hardcoded = np.array(\n",
    "    [\n",
    "        [0.9, -0.8, 0.3, -0.5],\n",
    "        [-0.4, 0.5, 0.1, -0.1],\n",
    "        [0.3, 0.1, -0.4, -0.8],\n",
    "        [-0.5, -0.2, -0.7, 0.5],\n",
    "    ]\n",
    ")\n",
    "sim_hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = sim_hardcoded\n",
    "b = sim.shape\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9,  0.5, -0.4,  0.5])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_diag = np.diag(sim)\n",
    "sim_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0.5,  0. ,  0. ],\n",
       "       [ 0. ,  0. , -0.4,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0.5]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_ap = np.diag(sim_diag)\n",
    "sim_ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_negative(m : np.array):\n",
    "    b = m.shape[0]\n",
    "    m_ap = np.diag(m)\n",
    "    m_an = m - np.diag(m_ap)\n",
    "    return np.sum(m_an, axis=1, keepdims=True) / (b - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33333333],\n",
       "       [-0.13333333],\n",
       "       [-0.13333333],\n",
       "       [-0.46666667]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_negative = get_mean_negative(sim)\n",
    "mean_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = sim.shape[0]\n",
    "np.identity(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_negative(m : np.array):\n",
    "    b = m.shape[0]\n",
    "    m_ap = np.diag(m)\n",
    "    m_an = m - np.diag(m_ap)\n",
    "    mask_1 = (np.identity(b) == 1)\n",
    "    mask_2 = m_an > m_ap.reshape(b, 1)\n",
    "    mask = (mask_1 | mask_2)\n",
    "    m_masked = np.copy(m)\n",
    "    m_masked[mask] = -2\n",
    "    return np.max(m_masked, axis=1, keepdims=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3],\n",
       "       [ 0.1],\n",
       "       [-0.8],\n",
       "       [-0.2]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest_negative = get_closest_negative(sim)\n",
    "closest_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Loss Functions\n",
    "\n",
    "The last step is to calculate the loss functions.\n",
    "\n",
    "$\\mathcal{L_\\mathrm{1}} = \\max{(mean\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
    "\n",
    "$\\mathcal{L_\\mathrm{2}} = \\max{(closest\\_neg -\\mathrm{s}(A,P)  +\\alpha, 0)}$\n",
    "\n",
    "$\\mathcal{L_\\mathrm{Full}} = \\mathcal{L_\\mathrm{1}} + \\mathcal{L_\\mathrm{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese_loss_function(m : np.array, alpha = 0.25):\n",
    "    b = m.shape[0]\n",
    "    mean_neg = get_mean_negative(m)\n",
    "    closest_neg = get_closest_negative(m)\n",
    "    sim_ap = np.diag(m)\n",
    "    \n",
    "    l1 = np.maximum(mean_neg - sim_ap.reshape(b, 1) + alpha, 0)\n",
    "    l2 = np.maximum(closest_neg - sim_ap.reshape(b, 1) + alpha, 0)\n",
    "    return l1 + l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.        ],\n",
       "       [0.51666667],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = siamese_loss_function(sim)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
