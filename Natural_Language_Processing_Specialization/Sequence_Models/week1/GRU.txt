Gated Recurrent Units

"Ants are really interesting. <They> are everywhere."

GRUs can predict as it learned to keep information about the subject.

 - Take two inputs at each step.
 - Take relevance and update state.

 Vanilla RNN

 - vanishing gradients problem: for long sequences the information tends to vanish.
 - GRU calculate more operations which takes more computation.


 Deep Recurrent Neural Networks

 - stacking GRUs together to create NN

 I was trying really hard to get a hold of ________. Louise, finally
 answered when I was about to give up.

 - Bi-directional RNN takes input from both directions.
 - follows acyclical graph.